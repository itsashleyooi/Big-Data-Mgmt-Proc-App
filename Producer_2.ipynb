{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT3182: Assignment 2 Part B (Event Producer 2) \n",
    "\n",
    "### Name: Ashley Ooi Yan-Lin (ID: 31171095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Processing Data Stream\n",
    "\n",
    "### (b) Write a python program that loads all the data from hotspot_AQUA_streaming.csv and randomly (with replacement) feed the data to the stream every 2 seconds. AQUA is the satellite from NASA that reports latitude, longitude, confidence and surface temperature of a location. You will need to append additional information such as producer information to identify the producer and created date & time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to establish a connection with our MongoClient and retrieve the collection created in Part A to obtain the most recent date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "client = MongoClient () \n",
    "db = client.fit3182_assignment_db\n",
    "collection = db.partA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create our program to facilitate the transmission of our data to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message published successfully. Data: {'latitude': -35.8125, 'longitude': 142.1286, 'confidence': 75.0, 'surface_temperature_celcius': 109.0, 'created_datetime': '2022-05-20 03:28:21', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -37.0827, 'longitude': 143.8836, 'confidence': 72.0, 'surface_temperature_celcius': 47.0, 'created_datetime': '2022-05-20 04:26:39', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -37.8343, 'longitude': 143.6581, 'confidence': 72.0, 'surface_temperature_celcius': 46.0, 'created_datetime': '2022-05-20 08:50:36', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -35.9438, 'longitude': 145.0824, 'confidence': 78.0, 'surface_temperature_celcius': 52.0, 'created_datetime': '2022-05-20 14:17:49', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -37.1929, 'longitude': 143.8132, 'confidence': 59.0, 'surface_temperature_celcius': 45.0, 'created_datetime': '2022-05-20 21:06:53', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -37.6745, 'longitude': 142.9848, 'confidence': 75.0, 'surface_temperature_celcius': 48.0, 'created_datetime': '2022-05-21 02:43:17', 'producer_id': 'hotspot_aqua_producer'}\n",
      "Message published successfully. Data: {'latitude': -36.0205, 'longitude': 146.4492, 'confidence': 71.0, 'surface_temperature_celcius': 46.0, 'created_datetime': '2022-05-21 04:31:43', 'producer_id': 'hotspot_aqua_producer'}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "\n",
    "# Reads data from hotspot_AQUA_streaming.csv, puts each row data into a document and appends all such documents into a list.\n",
    "def read_hotspot_AQUA_streaming():\n",
    "    hotspot_AQUA_streaming = pd.read_csv('hotspot_AQUA_streaming.csv')\n",
    "\n",
    "    data = []\n",
    "    for index,aquaRow in hotspot_AQUA_streaming.iterrows():\n",
    "        document = {}\n",
    "        document['latitude'] = float(aquaRow['latitude'])\n",
    "        document['longitude'] = float(aquaRow['longitude'])\n",
    "        document['confidence'] = int(aquaRow['confidence'])\n",
    "        document['surface_temperature_celcius'] = int(aquaRow['surface_temperature_celcius'])\n",
    "        data.append(document)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Gets the latest date in our collection\n",
    "def get_latest_date():\n",
    "    latest_date = collection.aggregate([\n",
    "                {\"$sort\":{\"date\":-1}},\n",
    "                {\"$project\":{\"_id\":0,\"date\":1}},\n",
    "                {\"$limit\":1}\n",
    "                ])\n",
    "    for document in latest_date:\n",
    "        latest_date = document['date']\n",
    "    return latest_date\n",
    "     \n",
    "# Publishes message to Kafka\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, value=data)\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'PartB'\n",
    "    print('Publishing records..')\n",
    "    producer02 = connect_kafka_producer()\n",
    "    data = read_hotspot_AQUA_streaming()  # Gets all the documents produced from hotspot_AQUA_streaming.csv\n",
    "    latest_date = get_latest_date() + timedelta(days=1) # After getting latest date, we would add one day to it to get the first date we would use to start feeding data\n",
    "    secondsPassed= 0 # Tracks how many seconds we should add to our latest date\n",
    "\n",
    "    while True:\n",
    "        chosenData = random.choice(data)  # Randomly chooses a document from our list of documents\n",
    "        curr_date = latest_date + timedelta(seconds=secondsPassed) # Creates the date we will use to feed our data by adding the number of seconds to our latest date from Part A\n",
    "        chosenData['producer'] = \"hotspot_AQUA_streaming\"\n",
    "        chosenData[\"created_datetime\"] = curr_date.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        publish_message(producer02, topic, chosenData)\n",
    "        secondsPassed += 17280 # After we insert a hotspot streaming data, add 17280 seconds to secondPassed. 17280 second is equivalent to 24 hours divided by 5. As we would insert 5 hotspot AQUA streaming data per day, adding 17280 seconds each time after we insert a hotspot streaming data simulates the time difference of adding the 5 data in a day\n",
    "        sleep(2) # Sleep for 2 seconds so that we would be able to insert 5 data per climate streaming data being inserted, as the climate streaming data is inserted every 10 seconds"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
